{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_MIeYqliFMJ"
   },
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hczzSwHSnTUc",
    "outputId": "d592dfb6-5dda-40c3-ee1b-5084f398e28d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  8 22:36:23 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "Your runtime has 13.6 gigabytes of available RAM\n",
      "\n",
      "Not using a high-RAM runtime\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0: print('Not connected to a GPU')\n",
    "else: print(gpu_info)\n",
    "print()\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "if ram_gb < 20: print('Not using a high-RAM runtime')\n",
    "else: print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDOhyMu7chCJ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import collections\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import PIL\n",
    "import copy\n",
    "from torch import utils\n",
    "from torch import optim\n",
    "from torch.utils import mobile_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0Ejd-JHiOqj"
   },
   "source": [
    "#Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gB3DC5bfOCjK"
   },
   "outputs": [],
   "source": [
    "# DATASET_NAME = 'fairset_good_227'\n",
    "DATASET_NAME = 'fairset_good_224'\n",
    "# DATASET_NAME = 'fairset_good_299'\n",
    "DRIVE_HOME = '/content/drive/MyDrive/sd_cloud_playground/cyberset_riko'\n",
    "\n",
    "DATASET_DIRECTORY = './{}'.format(DATASET_NAME)\n",
    "MODELS_DIRECTORY = '{}/models_pt'.format(DRIVE_HOME)\n",
    "# MODELS_DIRECTORY = './models_pt'\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.001\n",
    "LEARNING_MOMENTUM = 0.9\n",
    "DECAY_STEP = 7\n",
    "DECAY_GAMMA = 0.1\n",
    "TRAIN_EPOCHS = 25\n",
    "# TRAIN_EPOCHS = 20\n",
    "# TRAIN_EPOCHS = 40\n",
    "# TRAIN_EPOCHS = 1\n",
    "FAST_LEARNING = False\n",
    "\n",
    "# NET = 'alexnet'\n",
    "NET = 'resnet'\n",
    "# NET = 'inception'\n",
    "# NET = 'resnet50'\n",
    "QUANTIZATION = 'aware'\n",
    "# QUANTIZATION = 'static'\n",
    "# QUANTIZATION = 'dynamic'\n",
    "# QUANTIZATION = 'none'\n",
    "BACKEND_ENGINE = 'qnnpack'\n",
    "# BACKEND_ENGINE = 'fbgemm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yH_QCnspHkP4"
   },
   "source": [
    "#Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xo_PBKkpHjgo",
    "outputId": "4345073d-e6cb-48af-fe34-39ed2d59f893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "\n",
      "drive/:\n",
      "MyDrive\n",
      "\n",
      "fairset_good_224/:\n",
      "classes.txt  test  train  validation\n",
      "\n",
      "fairset_good_227/:\n",
      "classes.txt  test  train  validation\n",
      "\n",
      "fairset_good_299/:\n",
      "classes.txt  test  train  validation\n",
      "\n",
      "models_pt/:\n",
      "alexnet_fast_none_f32.pt    inception_fast_dynamic_f32.pt\n",
      "alexnet_full_none_f32.pt    inception_fast_dynamic_i8.ptl\n",
      "alexnet_full_static_f32.pt  inception_fast_none_f32.pt\n",
      "checkpoint_f32.pt\n",
      "\n",
      "sample_data/:\n",
      "anscombe.json\t\t      mnist_test.csv\n",
      "california_housing_test.csv   mnist_train_small.csv\n",
      "california_housing_train.csv  README.md\n"
     ]
    }
   ],
   "source": [
    "!cp -u {DRIVE_HOME}/{DATASET_NAME}.zip .\n",
    "!unzip -nq ./{DATASET_NAME}.zip\n",
    "!mkdir -p ./models_pt\n",
    "!pwd\n",
    "!echo\n",
    "!ls */"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nnBR0nge4bz"
   },
   "source": [
    "#Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wU0_40p_ZrdJ"
   },
   "outputs": [],
   "source": [
    "# show_image(image, label=predicted_class, label_color='red', info=correct_class, info_color='yellow', color=True)\n",
    "def show_image(image, label='', color=True, info='', label_color='yellow', info_color='yellow'):\n",
    "    colors={'yellow':((255,255,127)), 'red':(255,63,63), 'green':(63,255,63), 'blue':(63,63,255), 'mono':(224)}\n",
    "    picture = image.copy()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.axis('off')\n",
    "    if color:\n",
    "        cv2.putText(picture, label, (0,25), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,0), 6, cv2.LINE_AA)\n",
    "        cv2.putText(picture, label, (0,25), cv2.FONT_HERSHEY_SIMPLEX, 1,colors[label_color], 2, cv2.LINE_AA)\n",
    "        cv2.putText(picture, info, (0,55), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,0), 6, cv2.LINE_AA)\n",
    "        cv2.putText(picture, info, (0,55), cv2.FONT_HERSHEY_SIMPLEX, 1,colors[info_color], 2, cv2.LINE_AA)\n",
    "        plt.imshow(picture)\n",
    "    else:\n",
    "        cv2.putText(picture, label, (0,25), cv2.FONT_HERSHEY_SIMPLEX, 1,(0), 6, cv2.LINE_AA)\n",
    "        cv2.putText(picture, label, (0,25), cv2.FONT_HERSHEY_SIMPLEX, 1,colors['mono'], 2, cv2.LINE_AA)\n",
    "        plt.imshow(picture, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# dataloaders, classes = get_dataloaders(verbose=True, resize=0)\n",
    "def get_dataloaders(verbose=True, resize=0):\n",
    "    if resize > 0:\n",
    "        data_transform = tv.transforms.Compose([tv.transforms.Resize(resize), tv.transforms.ToTensor(),\n",
    "                                                tv.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    else:\n",
    "        data_transform = tv.transforms.Compose([tv.transforms.ToTensor(), tv.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    inner_directories = [x for x in os.listdir(DATASET_DIRECTORY) if os.path.isdir('{}/{}'.format(DATASET_DIRECTORY, x))]\n",
    "    datasets = {x: tv.datasets.ImageFolder(os.path.join(DATASET_DIRECTORY, x), data_transform) for x in inner_directories}\n",
    "    dataloaders = {x: utils.data.DataLoader(datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=2) for x in inner_directories}\n",
    "    classes = datasets[inner_directories[0]].classes\n",
    "    if verbose:\n",
    "        images_count = {x: len(datasets[x]) for x in inner_directories}\n",
    "        classes_count = {x: len(datasets[x].classes) for x in inner_directories}\n",
    "        print('IMAGES:')\n",
    "        for k, v in images_count.items():\n",
    "            print(v, k)\n",
    "        print()\n",
    "        print('CLASSES:')\n",
    "        for k, v in classes_count.items():\n",
    "            print(v, k)\n",
    "        print()\n",
    "    return dataloaders, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scMYdn2f3gLK"
   },
   "outputs": [],
   "source": [
    "# predicted_class = query_model(model, image_path, class_names, correct_label=None, resize=0, verbose=True, force_cpu=False)\n",
    "def query_model(model, image_path, class_names, correct_label=None, resize=0, verbose=True, force_cpu=False):\n",
    "    model.eval()\n",
    "    input_image = PIL.Image.open(image_path)\n",
    "    if resize > 0:\n",
    "        preprocess = tv.transforms.Compose([tv.transforms.Resize(resize), tv.transforms.ToTensor(),\n",
    "                                                tv.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    else:\n",
    "        preprocess = tv.transforms.Compose([tv.transforms.ToTensor(), tv.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    if force_cpu:\n",
    "        input_batch = input_batch.to('cpu')\n",
    "        model = model.to('cpu')\n",
    "    else:\n",
    "        input_batch = input_batch.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    top2_prob, top2_id = torch.topk(probabilities, 2)\n",
    "    predicted_class = classes_names[top2_id[0]]\n",
    "    second_guess = classes_names[top2_id[1]]\n",
    "    if verbose:\n",
    "        if correct_label != None:\n",
    "            correct_class = correct_label\n",
    "            if predicted_class == correct_class:\n",
    "                show_image(np.array(input_image), label='NILF{}'.format(predicted_class), label_color='green')\n",
    "                print('RIGHT_GUESS -> answer: http://nilf.it/{} | confidence: {:.1f}%\\n'.format(predicted_class, top2_prob[0]*100))\n",
    "            else:\n",
    "                show_image(np.array(input_image), label='NILF{}'.format(predicted_class), label_color='red',\n",
    "                           info='NILF{}'.format(correct_class), info_color='yellow')\n",
    "                print('WRONG_GUESS -> answer: http://nilf.it/{} | confidence: {:.1f}% | second: http://nilf.it/{} | truth: http://nilf.it/{}\\n'\n",
    "                      .format(predicted_class, top2_prob[0]*100, second_guess, correct_class))\n",
    "        else:\n",
    "            show_image(np.array(input_image), label='NILF{}'.format(predicted_class))\n",
    "            print('GUESS -> answer: http://nilf.it/{} | confidence: {:.1f}% | second: http://nilf.it/{}\\n'\n",
    "                  .format(predicted_class, top2_prob[0]*100, second_guess))\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuHiPczjei0U"
   },
   "source": [
    "#Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoZEBXVvneT3"
   },
   "outputs": [],
   "source": [
    "# model, criterion, optimizer, scheduler = build_model(classes_number, net='resnet', quantization='aware', verbose=True)\n",
    "def build_model(classes_number, net='alexnet', quantization='none', verbose=True):\n",
    "    if net == 'alexnet':\n",
    "        model = tv.models.alexnet(pretrained=True)\n",
    "        if FAST_LEARNING:\n",
    "            for p in model.parameters():\n",
    "                p.requires_grad = False\n",
    "        num_features = model.classifier[6].in_features\n",
    "        num_classes = classes_number\n",
    "        model.classifier[6] = torch.nn.Linear(num_features, num_classes)\n",
    "        model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        params = [p for (n,p) in model.named_parameters() if p.requires_grad]\n",
    "        params_names = [n for (n,p) in model.named_parameters() if p.requires_grad]\n",
    "        if verbose:\n",
    "            print('PARAMS:')\n",
    "            for p in params_names:\n",
    "                print(p)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(params, lr=LEARNING_RATE, momentum=LEARNING_MOMENTUM)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=DECAY_STEP, gamma=DECAY_GAMMA)\n",
    "        if quantization == 'static' or quantization == 'aware':\n",
    "            model = torch.nn.Sequential(collections.OrderedDict([('quant', torch.quantization.QuantStub()),\n",
    "                                                                ('alex', model),\n",
    "                                                                ('dequant', torch.quantization.DeQuantStub())]))\n",
    "            model.train()\n",
    "            model.qconfig = torch.quantization.get_default_qat_qconfig(BACKEND_ENGINE)\n",
    "            torch.backends.quantized.engine = BACKEND_ENGINE\n",
    "            fusing_modules = []\n",
    "            fusing_modules.append(['alex.features.0', 'alex.features.1'])\n",
    "            fusing_modules.append(['alex.features.3', 'alex.features.4'])\n",
    "            fusing_modules.append(['alex.features.6', 'alex.features.7'])\n",
    "            fusing_modules.append(['alex.features.8', 'alex.features.9'])\n",
    "            fusing_modules.append(['alex.features.10', 'alex.features.11'])\n",
    "            fusing_modules.append(['alex.classifier.1', 'alex.classifier.2'])\n",
    "            fusing_modules.append(['alex.classifier.4', 'alex.classifier.5'])\n",
    "            torch.quantization.fuse_modules(model, fusing_modules, inplace=True)\n",
    "            if quantization == 'aware':\n",
    "                torch.quantization.prepare_qat(model, inplace=True)\n",
    "    elif net == 'resnet':\n",
    "        if quantization == 'static' or quantization == 'aware':\n",
    "            model = tv.models.quantization.resnet18(pretrained=True)\n",
    "        else:\n",
    "            model = tv.models.resnet18(pretrained=True)\n",
    "        if FAST_LEARNING:\n",
    "            for p in model.parameters():\n",
    "                p.requires_grad = False\n",
    "        num_features = model.fc.in_features\n",
    "        num_classes = classes_number\n",
    "        model.fc = torch.nn.Linear(num_features, num_classes)\n",
    "        model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        params = [p for (n,p) in model.named_parameters() if p.requires_grad]\n",
    "        params_names = [n for (n,p) in model.named_parameters() if p.requires_grad]\n",
    "        if verbose:\n",
    "            print('PARAMS:')\n",
    "            for p in params_names:\n",
    "                print(p)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(params, lr=LEARNING_RATE, momentum=LEARNING_MOMENTUM)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=DECAY_STEP, gamma=DECAY_GAMMA)\n",
    "        if quantization == 'static' or quantization == 'aware':\n",
    "            model.train()\n",
    "            if quantization == 'aware':\n",
    "                model.qconfig = torch.quantization.get_default_qat_qconfig(BACKEND_ENGINE)\n",
    "                torch.backends.quantized.engine = BACKEND_ENGINE\n",
    "            fusing_modules = []\n",
    "            fusing_modules.append(['conv1', 'bn1', 'relu'])\n",
    "            fusing_modules.append(['layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu'])\n",
    "            fusing_modules.append(['layer1.0.conv2', 'layer1.0.bn2'])\n",
    "            fusing_modules.append(['layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu'])\n",
    "            fusing_modules.append(['layer1.1.conv2', 'layer1.1.bn2'])\n",
    "            fusing_modules.append(['layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu'])\n",
    "            fusing_modules.append(['layer2.0.conv2', 'layer2.0.bn2'])\n",
    "            fusing_modules.append(['layer2.0.downsample.0', 'layer2.0.downsample.1'])\n",
    "            fusing_modules.append(['layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu'])\n",
    "            fusing_modules.append(['layer2.1.conv2', 'layer2.1.bn2'])\n",
    "            fusing_modules.append(['layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu'])\n",
    "            fusing_modules.append(['layer3.0.conv2', 'layer3.0.bn2'])\n",
    "            fusing_modules.append(['layer3.0.downsample.0', 'layer3.0.downsample.1'])\n",
    "            fusing_modules.append(['layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu'])\n",
    "            fusing_modules.append(['layer3.1.conv2', 'layer3.1.bn2'])\n",
    "            fusing_modules.append(['layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu'])\n",
    "            fusing_modules.append(['layer4.0.conv2', 'layer4.0.bn2'])\n",
    "            fusing_modules.append(['layer4.0.downsample.0', 'layer4.0.downsample.1'])\n",
    "            fusing_modules.append(['layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu'])\n",
    "            fusing_modules.append(['layer4.1.conv2', 'layer4.1.bn2'])\n",
    "            torch.quantization.fuse_modules(model, fusing_modules, inplace=True)\n",
    "            if quantization == 'aware':\n",
    "                torch.quantization.prepare_qat(model, inplace=True)\n",
    "    elif net == 'inception':\n",
    "        model = tv.models.inception_v3(pretrained=True)\n",
    "        if FAST_LEARNING:\n",
    "            for p in model.parameters():\n",
    "                p.requires_grad = False\n",
    "        num_classes = classes_number\n",
    "        num_features = model.AuxLogits.fc.in_features\n",
    "        model.AuxLogits.fc = torch.nn.Linear(num_features, num_classes)\n",
    "        num_features_2 = model.fc.in_features\n",
    "        model.fc = torch.nn.Linear(num_features_2, num_classes)\n",
    "        model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        params = [p for (n,p) in model.named_parameters() if p.requires_grad]\n",
    "        params_names = [n for (n,p) in model.named_parameters() if p.requires_grad]\n",
    "        if verbose:\n",
    "            print('PARAMS:')\n",
    "            for p in params_names:\n",
    "                print(p)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(params, lr=LEARNING_RATE, momentum=LEARNING_MOMENTUM)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=DECAY_STEP, gamma=DECAY_GAMMA)\n",
    "        if quantization == 'static':\n",
    "            model = torch.nn.Sequential(collections.OrderedDict([('quant', torch.quantization.QuantStub()),\n",
    "                                                                ('inc', model),\n",
    "                                                                ('dequant', torch.quantization.DeQuantStub())]))\n",
    "        elif quantization == 'aware':\n",
    "            model = None\n",
    "            criterion = None\n",
    "            optimizer = None\n",
    "            scheduler = None\n",
    "    elif 'resnet50':\n",
    "        if quantization == 'static' or quantization == 'aware':\n",
    "            model = tv.models.quantization.resnet50(pretrained=True)\n",
    "        else:\n",
    "            model = tv.models.resnet50(pretrained=True)\n",
    "        if FAST_LEARNING:\n",
    "            for p in model.parameters():\n",
    "                p.requires_grad = False\n",
    "        num_features = model.fc.in_features\n",
    "        num_classes = classes_number\n",
    "        model.fc = torch.nn.Linear(num_features, num_classes)\n",
    "        model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        params = [p for (n,p) in model.named_parameters() if p.requires_grad]\n",
    "        params_names = [n for (n,p) in model.named_parameters() if p.requires_grad]\n",
    "        if verbose:\n",
    "            print('PARAMS:')\n",
    "            for p in params_names:\n",
    "                print(p)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(params, lr=LEARNING_RATE, momentum=LEARNING_MOMENTUM)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=DECAY_STEP, gamma=DECAY_GAMMA)\n",
    "        if quantization == 'aware':\n",
    "            model.train()\n",
    "            model.qconfig = torch.quantization.get_default_qat_qconfig(BACKEND_ENGINE)\n",
    "            torch.backends.quantized.engine = BACKEND_ENGINE\n",
    "            torch.quantization.prepare_qat(model, inplace=True)\n",
    "    else:\n",
    "        model = None\n",
    "        criterion = None\n",
    "        optimizer = None\n",
    "        scheduler = None\n",
    "    return model, criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oC4zhBsCRomn"
   },
   "outputs": [],
   "source": [
    "# model, weights = train_model(model, dataloaders, criterion, optimizer, scheduler, inception=True, checkpoint=False)\n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, inception=False, checkpoint=False):\n",
    "    since = time.time()\n",
    "    print('TRAINING_STARTED -> elapsed: {}'.format(datetime.timedelta(seconds=round(time.time()-since))))\n",
    "    best_accuracy = 0.0\n",
    "    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    weights = {'best': copy.deepcopy(model.state_dict()), 'last': copy.deepcopy(model.state_dict())}\n",
    "    for epoch in range(TRAIN_EPOCHS):\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                labels = labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_accuracy = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            print('{} -> epoch: {}/{} | loss: {:.3} | accuracy: {:.3} | elapsed: {}'\n",
    "                  .format(phase.upper(), epoch+1, TRAIN_EPOCHS, epoch_loss, epoch_accuracy, datetime.timedelta(seconds=round(time.time()-since))))\n",
    "            if phase == 'train':\n",
    "                weights['last'] = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'validation' and epoch_accuracy > best_accuracy:\n",
    "                best_accuracy = epoch_accuracy\n",
    "                weights['best'] = copy.deepcopy(model.state_dict())\n",
    "                save_weights(model, 'checkpoint')\n",
    "    print('TRAINING_COMPLETE -> elapsed: {}'.format(datetime.timedelta(seconds=round(time.time()-since))))\n",
    "    phase = 'test'\n",
    "    print('TESTING_STARTED -> elapsed: {}'.format(datetime.timedelta(seconds=round(time.time()-since))))\n",
    "    for state in ['best', 'last']:\n",
    "        model.load_state_dict(weights[state])\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            labels = labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(predictions == labels.data)\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_accuracy = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "        print('{} -> model: {} | loss: {:.3} | accuracy: {:.3} | elapsed: {}'\n",
    "              .format(phase.upper(), state, epoch_loss, epoch_accuracy, datetime.timedelta(seconds=round(time.time()-since))))\n",
    "    model.load_state_dict(weights['best'])\n",
    "    model.eval()\n",
    "    print('TESTING_COMPLETE -> elapsed: {}'.format(datetime.timedelta(seconds=round(time.time()-since))))\n",
    "    return model, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fkRcwAgqTr_"
   },
   "outputs": [],
   "source": [
    "# test_model(model, dataloaders['test'], classes_names, verbose=True, force_cpu=False, times=1)\n",
    "def test_model(model, dataloader, class_names, verbose=True, force_cpu=False, times=1):\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "    for i in range(times):\n",
    "        right = 0\n",
    "        total = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            with torch.no_grad():\n",
    "                if force_cpu:\n",
    "                    inputs = inputs.to('cpu')\n",
    "                    labels = labels.to('cpu')\n",
    "                    model = model.to('cpu')\n",
    "                else:\n",
    "                    inputs = inputs.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    labels = labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                outputs = model(inputs)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                predicted_class = class_names[predictions[0]]\n",
    "                correct_class = class_names[labels[0]]\n",
    "                if predicted_class == correct_class:\n",
    "                    right += 1\n",
    "                elif verbose:\n",
    "                    print('TEST -> error: http://nilf.it/{} | truth: http://nilf.it/{}'.format(predicted_class, correct_class))\n",
    "                total += 1\n",
    "        print('TEST -> correct: {}/{} | accuracy: {:.1f}% | elapsed: {}'.format(right, total, right/total*100, \n",
    "                                                                                datetime.timedelta(seconds=round(time.time()-since))))\n",
    "        accuracies.append(right/total*100)\n",
    "    if times > 1:\n",
    "         print('TEST -> average: {:.1f}% | elapsed: {}'.format(np.mean(accuracies), datetime.timedelta(seconds=round(time.time()-since))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzcyJCCpg2Rr"
   },
   "outputs": [],
   "source": [
    "# save_weights(model, name, torchscript=False, lite=False)\n",
    "def save_weights(model, name, torchscript=False, lite=False):\n",
    "    if lite:\n",
    "        model._save_for_lite_interpreter(os.path.join(MODELS_DIRECTORY, '{}_i8.ptl'.format(name)))\n",
    "    elif torchscript:\n",
    "        torch.jit.save(model, os.path.join(MODELS_DIRECTORY, '{}_i8.pt'.format(name)))\n",
    "    else:\n",
    "        torch.save(model.state_dict(), os.path.join(MODELS_DIRECTORY, '{}_f32.pt'.format(name)))\n",
    "\n",
    "# model = load_weights(model, name, torchscript=False, lite=False)\n",
    "def load_weights(model, name, torchscript=False, lite=False):\n",
    "    if lite:\n",
    "        model = torch.jit.load(os.path.join(MODELS_DIRECTORY, '{}_i8.ptl'.format(name)))\n",
    "    elif torchscript:\n",
    "        model = torch.jit.load(os.path.join(MODELS_DIRECTORY, '{}_i8.pt'.format(name)))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(os.path.join(MODELS_DIRECTORY, '{}_f32.pt'.format(name))))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# script_model = quantize_model(model, quantization='aware')\n",
    "def quantize_model(model, quantization='aware'):\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    if quantization == 'aware':\n",
    "        quantized_model = torch.quantization.convert(model, inplace=False)\n",
    "    elif quantization == 'static':\n",
    "        model.qconfig = torch.quantization.get_default_qconfig(BACKEND_ENGINE)\n",
    "        torch.backends.quantized.engine = BACKEND_ENGINE\n",
    "        model = torch.quantization.prepare(model, inplace=True)\n",
    "        quantized_model = torch.quantization.convert(model, inplace=False)\n",
    "    elif quantization == 'dynamic':\n",
    "        quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8, inplace=False)\n",
    "    script_model = torch.jit.script(quantized_model)\n",
    "    script_model = mobile_optimizer.optimize_for_mobile(script_model)\n",
    "    return script_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7nXzBpfbCKM"
   },
   "source": [
    "#Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz--C387ey_R"
   },
   "outputs": [],
   "source": [
    "# show_image(image, label=predicted_class, label_color='red', info=correct_class, info_color='yellow', color=True)\n",
    "# dataloaders, classes = get_dataloaders(verbose=True, resize=0)\n",
    "# predicted_class = query_model(model, image_path, class_names, correct_label=None, resize=0, verbose=True, force_cpu=False)\n",
    "# model, criterion, optimizer, scheduler = build_model(classes_number, net='resnet', quantization='aware', verbose=True)\n",
    "# model, weights = train_model(model, dataloaders, criterion, optimizer, scheduler, inception=True, checkpoint=False)\n",
    "# test_model(model, dataloaders['test'], classes_names, verbose=True, force_cpu=False, times=1)\n",
    "# save_weights(model, name, torchscript=False, lite=False)\n",
    "# model = load_weights(model, name, torchscript=False, lite=False)\n",
    "# script_model = quantize_model(model, quantization='aware')\n",
    "\n",
    "# test_images = []\n",
    "# test_labels = []\n",
    "# for label in os.listdir(os.path.join(DATASET_DIRECTORY, 'test')):\n",
    "#     for image in os.listdir(os.path.join(DATASET_DIRECTORY, 'test', label)):\n",
    "#         test_images.append(os.path.join(DATASET_DIRECTORY, 'test', label, image))\n",
    "#         test_labels.append(label)\n",
    "# test_set = list(zip(test_images, test_labels))\n",
    "# random.shuffle(test_set)\n",
    "# for i, l in test_set:\n",
    "#     query_model(model, i, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoLgMFH3g-Zh"
   },
   "source": [
    "#Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cv5LWlCPBqGt",
    "outputId": "eeccfd43-4888-4439-c8f8-21383594babc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGES:\n",
      "975 validation\n",
      "695 test\n",
      "39360 train\n",
      "\n",
      "CLASSES:\n",
      "656 validation\n",
      "656 test\n",
      "656 train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataloaders, classes = get_dataloaders()\n",
    "suffix = 'fast' if FAST_LEARNING else 'full'\n",
    "pt_lite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4cFULzsWwEv"
   },
   "outputs": [],
   "source": [
    "# model, criterion, optimizer, scheduler = build_model(len(classes), net=NET, quantization=QUANTIZATION, verbose=True)\n",
    "# model, weights = train_model(model, dataloaders, criterion, optimizer, scheduler, inception=(NET=='inception'), checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4y4CR-UMFBn"
   },
   "outputs": [],
   "source": [
    "# save_weights(model, '{}_{}_{}'.format(NET, suffix, QUANTIZATION), torchscript=False, lite=False)\n",
    "# script_model = quantize_model(model, quantization=QUANTIZATION)\n",
    "# save_weights(script_model, '{}_{}_{}'.format(NET, suffix, QUANTIZATION), torchscript=True, lite=pt_lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKAkF5r4Lj5J"
   },
   "outputs": [],
   "source": [
    "# loaded_model, _, _, _ = build_model(len(classes), net=NET, quantization=QUANTIZATION, verbose=False)\n",
    "# loaded_model = load_weights(loaded_model, '{}_{}_{}'.format(NET, suffix, QUANTIZATION), torchscript=False, lite=False)\n",
    "lite_model = load_weights(_, '{}_{}_{}'.format(NET, suffix, QUANTIZATION), torchscript=True, lite=pt_lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y34c9BYidAy2",
    "outputId": "dc567cf5-cad7-44d4-f2d9-919b5cb85c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -> error: http://nilf.it/109609 | truth: http://nilf.it/107093\n",
      "TEST -> error: http://nilf.it/109400 | truth: http://nilf.it/107075\n",
      "TEST -> error: http://nilf.it/101009 | truth: http://nilf.it/103257\n",
      "TEST -> error: http://nilf.it/114283 | truth: http://nilf.it/114064\n",
      "TEST -> error: http://nilf.it/119806 | truth: http://nilf.it/119901\n",
      "TEST -> error: http://nilf.it/104451 | truth: http://nilf.it/107217\n",
      "TEST -> correct: 168/174 | accuracy: 96.6% | elapsed: 0:00:39\n",
      "TEST -> error: http://nilf.it/109400 | truth: http://nilf.it/107075\n",
      "TEST -> error: http://nilf.it/104451 | truth: http://nilf.it/107217\n",
      "TEST -> error: http://nilf.it/119806 | truth: http://nilf.it/119901\n",
      "TEST -> error: http://nilf.it/106148 | truth: http://nilf.it/104454\n",
      "TEST -> error: http://nilf.it/109609 | truth: http://nilf.it/107093\n",
      "TEST -> correct: 169/174 | accuracy: 97.1% | elapsed: 0:01:17\n",
      "TEST -> error: http://nilf.it/111482 | truth: http://nilf.it/113944\n",
      "TEST -> error: http://nilf.it/106204 | truth: http://nilf.it/101517\n",
      "TEST -> error: http://nilf.it/104370 | truth: http://nilf.it/102097\n",
      "TEST -> correct: 171/174 | accuracy: 98.3% | elapsed: 0:01:56\n",
      "TEST -> error: http://nilf.it/106148 | truth: http://nilf.it/104454\n",
      "TEST -> error: http://nilf.it/106204 | truth: http://nilf.it/101517\n",
      "TEST -> error: http://nilf.it/109400 | truth: http://nilf.it/107075\n",
      "TEST -> correct: 171/174 | accuracy: 98.3% | elapsed: 0:02:35\n",
      "TEST -> error: http://nilf.it/119806 | truth: http://nilf.it/119901\n",
      "TEST -> error: http://nilf.it/109400 | truth: http://nilf.it/107075\n",
      "TEST -> error: http://nilf.it/122739 | truth: http://nilf.it/109708\n",
      "TEST -> error: http://nilf.it/111482 | truth: http://nilf.it/113944\n",
      "TEST -> correct: 170/174 | accuracy: 97.7% | elapsed: 0:03:14\n",
      "TEST -> error: http://nilf.it/109400 | truth: http://nilf.it/107075\n",
      "TEST -> error: http://nilf.it/109609 | truth: http://nilf.it/107093\n",
      "TEST -> error: http://nilf.it/101009 | truth: http://nilf.it/103257\n",
      "TEST -> error: http://nilf.it/122739 | truth: http://nilf.it/109708\n",
      "TEST -> correct: 170/174 | accuracy: 97.7% | elapsed: 0:03:53\n",
      "TEST -> error: http://nilf.it/106204 | truth: http://nilf.it/101517\n",
      "TEST -> correct: 173/174 | accuracy: 99.4% | elapsed: 0:04:32\n",
      "TEST -> error: http://nilf.it/122739 | truth: http://nilf.it/109708\n",
      "TEST -> error: http://nilf.it/109400 | truth: http://nilf.it/107075\n",
      "TEST -> error: http://nilf.it/106204 | truth: http://nilf.it/101517\n",
      "TEST -> correct: 171/174 | accuracy: 98.3% | elapsed: 0:05:11\n",
      "TEST -> error: http://nilf.it/122739 | truth: http://nilf.it/109708\n",
      "TEST -> error: http://nilf.it/106204 | truth: http://nilf.it/101517\n",
      "TEST -> error: http://nilf.it/114283 | truth: http://nilf.it/114064\n",
      "TEST -> correct: 171/174 | accuracy: 98.3% | elapsed: 0:05:50\n",
      "TEST -> error: http://nilf.it/106204 | truth: http://nilf.it/101517\n",
      "TEST -> error: http://nilf.it/104451 | truth: http://nilf.it/107217\n",
      "TEST -> correct: 172/174 | accuracy: 98.9% | elapsed: 0:06:29\n",
      "TEST -> average: 98.0% | elapsed: 0:06:29\n"
     ]
    }
   ],
   "source": [
    "# test_model(loaded_model, dataloaders['test'], classes, force_cpu=False, verbose=False, times=10)\n",
    "test_model(lite_model, dataloaders['test'], classes, force_cpu=True, verbose=True, times=10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "x_MIeYqliFMJ",
    "yH_QCnspHkP4",
    "-nnBR0nge4bz"
   ],
   "name": "cybernet_pytorch_transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
