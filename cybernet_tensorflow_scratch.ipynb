{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cybernet_tensorflow_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqY7W0VV2jK2"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%matplotlib inline\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "!cp -u /content/drive/MyDrive/sd_cloud_playground/cyberset_riko/cyberset_tf_64.zip .\n",
        "!unzip -nq ./cyberset_tf_64.zip\n",
        "!mkdir -p models_tf/last\n",
        "!mkdir -p models_tf/best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd_ys6CbSwRj"
      },
      "source": [
        "def show_image(image, label='', color=True):\n",
        "    picture = image.copy()\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.axis('off')\n",
        "    if color:\n",
        "        cv2.putText(picture, label, (0,25), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,0), 6, cv2.LINE_AA)\n",
        "        cv2.putText(picture, label, (0,25), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,255,0), 2, cv2.LINE_AA)\n",
        "        plt.imshow(picture)\n",
        "    else:\n",
        "        cv2.putText(picture, label, (0,25), cv2.FONT_HERSHEY_SIMPLEX, 1,(0), 6, cv2.LINE_AA)\n",
        "        cv2.putText(picture, label, (0,25), cv2.FONT_HERSHEY_SIMPLEX, 1,(255), 2, cv2.LINE_AA)\n",
        "        plt.imshow(picture, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "def load_dataset(directory):\n",
        "    images = ['{}/images/{}'.format(directory, x) for x in os.listdir('{}/images'.format(directory))]\n",
        "    images.sort()\n",
        "    with open('{}/labels.txt'.format(directory), 'r') as f:\n",
        "        labels = f.read().splitlines()\n",
        "    return images, labels\n",
        "\n",
        "def get_labels_encoder(labels):\n",
        "    reduced = list(set(labels))\n",
        "    reduced.sort()\n",
        "    encoder_dictionary = {}\n",
        "    decoder_dictionary = {}\n",
        "    for i, l in enumerate(reduced):\n",
        "        code = [0.0 for j in range(0, i)] + [1.0] + [0.0 for j in range(i, len(reduced)-1)]\n",
        "        encoder_dictionary[l] = code\n",
        "        decoder_dictionary[np.argmax(code, axis=-1)] = l\n",
        "    def encoder(label):\n",
        "        return encoder_dictionary[label]\n",
        "    def decoder(node):\n",
        "        return decoder_dictionary[node]\n",
        "    return encoder, decoder, len(encoder_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciHyp5h2XaQV"
      },
      "source": [
        "TRAIN_DIRECTORY = './cyberset_tf_64/train'\n",
        "VALIDATION_DIRECTORY = './cyberset_tf_64/validation'\n",
        "TEST_DIRECTORY = './cyberset_tf_64/test'\n",
        "# MODELS_DIRECTORY = '/content/drive/MyDrive/sd_cloud_playground/cyberset_riko/models_tf'\n",
        "MODELS_DIRECTORY = './models_tf'\n",
        "BATCH_SIZE = 163\n",
        "# LEARNING_RATE = 1e-3\n",
        "LEARNING_RATE = 0.0001\n",
        "# TRAINING_STEPS = 54000\n",
        "# TRAINING_STEPS = 10000\n",
        "TRAINING_STEPS = 30000\n",
        "VALIDATION_STEP = 2000\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS = 3\n",
        "# BILATERAL_SIZE = 9\n",
        "# BILATERAL_SIGMA = 75\n",
        "\n",
        "train_images, train_labels = load_dataset(TRAIN_DIRECTORY)\n",
        "validation_images, validation_labels = load_dataset(VALIDATION_DIRECTORY)\n",
        "test_images, test_labels = load_dataset(TEST_DIRECTORY)\n",
        "labels_encoder, labels_decoder, labels_count = get_labels_encoder(train_labels + validation_labels + test_labels)\n",
        "last_model = '{}/last/last'.format(MODELS_DIRECTORY)\n",
        "best_model = '{}/best/best'.format(MODELS_DIRECTORY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyYM9s73a87q"
      },
      "source": [
        "def prepare_image(path, color=True):\n",
        "    if color:\n",
        "        image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "    else:\n",
        "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    # image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    # image = cv2.bilateralFilter(image, BILATERAL_SIZE, BILATERAL_SIGMA, BILATERAL_SIGMA)\n",
        "    return image\n",
        "\n",
        "def get_train_batch(images_list, labels_list, labels_encoder):\n",
        "    images = list()\n",
        "    labels = list()\n",
        "    for i in range(BATCH_SIZE):\n",
        "        index = random.randint(0, len(labels_list)-1)\n",
        "        if CHANNELS > 1:\n",
        "            image = prepare_image(images_list[index]) / 255.0\n",
        "        else:\n",
        "            image = np.expand_dims(prepare_image(images_list[index], color=False)/255.0, axis=-1)\n",
        "        label = labels_encoder(labels_list[index])\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "    return (np.array(images), np.array(labels))\n",
        "\n",
        "def get_validation_batch(images_list, labels_list, labels_encoder, offset):\n",
        "    images = list()\n",
        "    labels = list()\n",
        "    for i in range(offset, offset+BATCH_SIZE):\n",
        "        index = random.randint(0, len(labels_list)-1)\n",
        "        if CHANNELS > 1:\n",
        "            image = prepare_image(images_list[index]) / 255.0\n",
        "        else:\n",
        "            image = np.expand_dims(prepare_image(images_list[index], color=False)/255.0, axis=-1)\n",
        "        label = labels_encoder(labels_list[index])\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "    return (np.array(images), np.array(labels))\n",
        "\n",
        "def get_test_batch(images_list, labels_list, labels_encoder, index):\n",
        "    images = list()\n",
        "    labels = list()\n",
        "    if CHANNELS > 1:\n",
        "        image = prepare_image(images_list[index]) / 255.0\n",
        "    else:\n",
        "        image = np.expand_dims(prepare_image(images_list[index], color=False)/255.0, axis=-1)\n",
        "    label = labels_encoder(labels_list[index])\n",
        "    images.append(image)\n",
        "    labels.append(label)\n",
        "    return (np.array(images), np.array(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQm3xLbsREcY"
      },
      "source": [
        "def convolve_2d(input_node, kernel_shape, bias_shape, stride=1, padding='VALID', name='convolution'):\n",
        "    with tf.variable_scope(name):\n",
        "        weights = tf.get_variable('weights', kernel_shape, initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float32)\n",
        "        biases = tf.get_variable('biases', bias_shape, initializer=tf.truncated_normal_initializer(), dtype=tf.float32)\n",
        "        result = tf.nn.conv2d(input_node, weights, strides=[1,stride,stride,1], padding=padding)\n",
        "        output = tf.nn.bias_add(result, biases)\n",
        "    return output\n",
        "\n",
        "# 128x128x3\n",
        "'''def build_network(input, labels_count, name='building',\n",
        "                  c1_kernel=5, c1_layers=6, p1_size=2, p1_stride=2,\n",
        "                  c2_kernel=5, c2_layers=16, p2_size=2, p2_stride=2,\n",
        "                  f1_neurons=120, f2_neurons=84):\n",
        "    with tf.variable_scope(name):\n",
        "        batch = input.get_shape().as_list()[0]\n",
        "        conv1 = convolve_2d(input, kernel_shape=[c1_kernel,c1_kernel,CHANNELS,c1_layers], bias_shape=[c1_layers], name='conv1')\n",
        "        norm1 = tf.contrib.layers.batch_norm(conv1)\n",
        "        act1 =  tf.nn.relu(norm1, name='act1')\n",
        "        pool1 = tf.nn.max_pool(act1, ksize=[1,p1_size,p1_size,1], strides=[1,p1_stride,p1_stride,1], padding='VALID', name='pool1')\n",
        "        conv2 = convolve_2d(pool1, kernel_shape=[c2_kernel,c2_kernel,c1_layers,c2_layers], bias_shape=[c2_layers], name='conv2')\n",
        "        norm2 = tf.contrib.layers.batch_norm(conv2)\n",
        "        act2 =  tf.nn.relu(norm2, name='act2')\n",
        "        pool2 = tf.nn.max_pool(act2,ksize=[1,p2_size,p2_size,1], strides=[1,p2_stride,p2_stride,1], padding='VALID', name='pool2')\n",
        "        reshap = tf.reshape(pool2,[batch, -1])\n",
        "        full1 = tf.contrib.layers.fully_connected(reshap, num_outputs=f1_neurons, activation_fn=tf.nn.relu)\n",
        "        full2 = tf.contrib.layers.fully_connected(full1, num_outputs=f1_neurons, activation_fn=tf.nn.relu)\n",
        "        output = tf.contrib.layers.fully_connected(full2, num_outputs=labels_count, activation_fn=None)\n",
        "        prediction = tf.squeeze(tf.argmax(output, axis=-1))\n",
        "    return output, prediction'''\n",
        "\n",
        "# 56x56 batch 200\n",
        "# 32 64 128 nodes\n",
        "# filter 5x5x1 stride 1\n",
        "# maxpool 2x2 stride 1\n",
        "# 1024 fc, softmax\n",
        "# relu, lr 0.0001\n",
        "# dropout keep 0.5\n",
        "# 30 000 iterations with adam\n",
        "'''def build_network(input, labels_count, name='building',\n",
        "                  c1_kernel=5, c1_layers=32, p1_size=2, p1_stride=1,\n",
        "                  c2_kernel=5, c2_layers=64, p2_size=2, p2_stride=1,\n",
        "                  c3_kernel=5, c3_layers=128, p3_size=2, p3_stride=1,\n",
        "                  f1_neurons=1024):\n",
        "    with tf.variable_scope(name):\n",
        "        batch = input.get_shape().as_list()[0]\n",
        "        conv1 = convolve_2d(input, kernel_shape=[c1_kernel,c1_kernel,CHANNELS,c1_layers], bias_shape=[c1_layers], name='conv1')\n",
        "        act1 =  tf.nn.relu(conv1, name='act1')\n",
        "        pool1 = tf.nn.max_pool(act1, ksize=[1,p1_size,p1_size,1], strides=[1,p1_stride,p1_stride,1], padding='VALID', name='pool1')\n",
        "        conv2 = convolve_2d(pool1, kernel_shape=[c2_kernel,c2_kernel,c1_layers,c2_layers], bias_shape=[c2_layers], name='conv2')\n",
        "        act2 =  tf.nn.relu(conv2, name='act2')\n",
        "        pool2 = tf.nn.max_pool(act2,ksize=[1,p2_size,p2_size,1], strides=[1,p2_stride,p2_stride,1], padding='VALID', name='pool2')\n",
        "        conv3 = convolve_2d(pool2, kernel_shape=[c3_kernel,c3_kernel,c2_layers,c3_layers], bias_shape=[c3_layers], name='conv3')\n",
        "        act3 =  tf.nn.relu(conv3, name='act3')\n",
        "        pool3 = tf.nn.max_pool(act3,ksize=[1,p3_size,p3_size,1], strides=[1,p3_stride,p3_stride,1], padding='VALID', name='pool3')\n",
        "        reshap = tf.reshape(pool3,[batch, -1])\n",
        "        full1 = tf.contrib.layers.fully_connected(reshap, num_outputs=f1_neurons, activation_fn=tf.nn.relu)\n",
        "        drop1 = tf.contrib.layers.dropout(full1, keep_prob=0.5)\n",
        "        output = tf.contrib.layers.fully_connected(drop1, num_outputs=labels_count, activation_fn=None)\n",
        "        prediction = tf.squeeze(tf.argmax(output, axis=-1))\n",
        "    return output, prediction'''\n",
        "\n",
        "# 3 conv layers\n",
        "# each w/ 2x2 max_pool non_overlapping\n",
        "# each w/ 3x3 kernel\n",
        "# feature maps 16 32 64\n",
        "# convolution stride 1\n",
        "# relu everywhere except last\n",
        "# 2 full 256 neurons and softmax 14 (classes)\n",
        "# RMSProp update rule\n",
        "# covers 64x96\n",
        "def build_network(input, labels_count, name='building',\n",
        "                  c1_kernel=3, c1_layers=16, p1_size=2, p1_stride=2,\n",
        "                  c2_kernel=3, c2_layers=32, p2_size=2, p2_stride=2,\n",
        "                  c3_kernel=3, c3_layers=64, p3_size=2, p3_stride=2,\n",
        "                  f1_neurons=256):\n",
        "    with tf.variable_scope(name):\n",
        "        batch = input.get_shape().as_list()[0]\n",
        "        conv1 = convolve_2d(input, kernel_shape=[c1_kernel,c1_kernel,CHANNELS,c1_layers], bias_shape=[c1_layers], name='conv1')\n",
        "        norm1 = tf.contrib.layers.batch_norm(conv1)\n",
        "        act1 =  tf.nn.relu(norm1, name='act1')\n",
        "        pool1 = tf.nn.max_pool(act1, ksize=[1,p1_size,p1_size,1], strides=[1,p1_stride,p1_stride,1], padding='VALID', name='pool1')\n",
        "        conv2 = convolve_2d(pool1, kernel_shape=[c2_kernel,c2_kernel,c1_layers,c2_layers], bias_shape=[c2_layers], name='conv2')\n",
        "        norm2 = tf.contrib.layers.batch_norm(conv2)\n",
        "        act2 =  tf.nn.relu(norm2, name='act2')\n",
        "        pool2 = tf.nn.max_pool(act2,ksize=[1,p2_size,p2_size,1], strides=[1,p2_stride,p2_stride,1], padding='VALID', name='pool2')\n",
        "        conv3 = convolve_2d(pool2, kernel_shape=[c3_kernel,c3_kernel,c2_layers,c3_layers], bias_shape=[c3_layers], name='conv3')\n",
        "        norm3 = tf.contrib.layers.batch_norm(conv3)\n",
        "        act3 =  tf.nn.relu(norm3, name='act3')\n",
        "        pool3 = tf.nn.max_pool(act3,ksize=[1,p3_size,p3_size,1], strides=[1,p3_stride,p3_stride,1], padding='VALID', name='pool3')\n",
        "        reshap = tf.reshape(pool3,[batch, -1])\n",
        "        full1 = tf.contrib.layers.fully_connected(reshap, num_outputs=f1_neurons, activation_fn=tf.nn.relu)\n",
        "        drop1 = tf.contrib.layers.dropout(full1, keep_prob=0.5)\n",
        "        output = tf.contrib.layers.fully_connected(drop1, num_outputs=labels_count, activation_fn=None)\n",
        "        prediction = tf.squeeze(tf.argmax(output, axis=-1))\n",
        "    return output, prediction\n",
        "\n",
        "def loss_function(output, labels, name='loss'):\n",
        "    with tf.variable_scope(name):\n",
        "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=labels)\n",
        "        loss = tf.reduce_mean(cross_entropy)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oveZiSUOgFEL"
      },
      "source": [
        "def train_network(train_images, train_labels, labels_count, validation_images, validation_labels, labels_encoder,\n",
        "                  start_from=0, last_accuracy=0):\n",
        "    tf.reset_default_graph()\n",
        "    input = tf.placeholder(tf.float32, shape=[BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE,CHANNELS], name='input')\n",
        "    labels = tf.placeholder(tf.float32, shape=[BATCH_SIZE,labels_count], name='labels')\n",
        "    output, prediction = build_network(input, labels_count)\n",
        "    loss = loss_function(output, labels)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
        "    optimization = optimizer.minimize(loss)\n",
        "    saver = tf.train.Saver()\n",
        "    params = 0\n",
        "    for v in tf.trainable_variables():\n",
        "        params += np.array(v.get_shape().as_list()).prod()\n",
        "    print('TRAINING STARTED -> params: {}'.format(params))\n",
        "    config = tf.ConfigProto(allow_soft_placement=True)\n",
        "    start_time = time.time()\n",
        "    best_accuracy = last_accuracy\n",
        "    with tf.Session(config=config) as session:\n",
        "        if start_from == 0:\n",
        "            session.run(tf.global_variables_initializer())\n",
        "            session.run(tf.local_variables_initializer())\n",
        "        else:\n",
        "            loader = tf.train.Saver()\n",
        "            loader.restore(session, '{}/last/last'.format(MODELS_DIRECTORY))\n",
        "        for s in range(start_from, TRAINING_STEPS):\n",
        "            train_batch = get_train_batch(train_images, train_labels, labels_encoder)\n",
        "            loss_value, _ = session.run([loss,optimization], feed_dict={input:train_batch[0], labels:train_batch[1]})\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print_frequency = TRAINING_STEPS // 100 if TRAINING_STEPS // 100 >= 10 else TRAINING_STEPS // 10 if TRAINING_STEPS // 10 >= 1 else 1\n",
        "            s_plus = s + 1\n",
        "            if s_plus % print_frequency == 0:\n",
        "                elapsed_delta = datetime.timedelta(seconds=round(elapsed_time))\n",
        "                print('TRAINING -> step: {}/{} | loss: {:.3} | elapsed: {}'.format(s_plus, TRAINING_STEPS, loss_value, elapsed_delta))\n",
        "                saver.save(session, '{}/last/last'.format(MODELS_DIRECTORY))\n",
        "                with open('{}/checkpoint_last.txt'.format(MODELS_DIRECTORY), 'w') as c:\n",
        "                    c.write('{}\\n'.format(s))\n",
        "            if VALIDATION_STEP > 0 and s_plus % VALIDATION_STEP == 0:\n",
        "                good_predictions = 0\n",
        "                for v in range(0, len(validation_labels), BATCH_SIZE):\n",
        "                    validation_batch = get_validation_batch(validation_images, validation_labels, labels_encoder, v)\n",
        "                    predicted_value, loss_value = session.run([prediction,loss], feed_dict={input:validation_batch[0], labels:validation_batch[1]})\n",
        "                    ground_truths = np.argmax(validation_batch[1], axis=-1)\n",
        "                    good_predictions += (predicted_value == ground_truths).sum()\n",
        "                print('VALIDATION -> correct: {}/{} | loss: {:.3} | accuracy: {:.3}'.format(good_predictions, len(validation_labels),\n",
        "                                                                                            loss_value, good_predictions/len(validation_labels)))\n",
        "                if good_predictions >= best_accuracy:\n",
        "                    best_accuracy = good_predictions\n",
        "                    saver.save(session, '{}/best/best'.format(MODELS_DIRECTORY))\n",
        "                    with open('{}/checkpoint_best.txt'.format(MODELS_DIRECTORY), 'w') as c:\n",
        "                        c.write('{}\\n'.format(best_accuracy))\n",
        "    end_time = time.time() - start_time\n",
        "    end_delta = datetime.timedelta(seconds=round(end_time))\n",
        "    print('TRAINING ENDED -> elapsed: {}'.format(end_delta))\n",
        "\n",
        "def resume_training(train_images, train_labels, labels_count, validation_images, validation_labels, labels_encoder):\n",
        "    with open('{}/checkpoint_last.txt'.format(MODELS_DIRECTORY), 'r') as c:\n",
        "        lines = c.read().splitlines()\n",
        "    if len(lines) > 0 and lines[0] != '':\n",
        "        step = int(lines[0]) + 1\n",
        "        with open('{}/checkpoint_best.txt'.format(MODELS_DIRECTORY), 'r') as c:\n",
        "            lines = c.read().splitlines()\n",
        "        if len(lines) > 0 and lines[0] != '':\n",
        "            accuracy = int(lines[0])\n",
        "            print('RESUMING TRAINING -> last: {}/{} | best: {}/{}'.format(step, TRAINING_STEPS, accuracy, len(validation_labels)))\n",
        "            train_network(train_images, train_labels, labels_count, validation_images, validation_labels, labels_encoder,\n",
        "                          start_from=step, last_accuracy=accuracy)\n",
        "        else:\n",
        "            print('RESUMING TRAINING -> failed')\n",
        "    else:\n",
        "        print('RESUMING TRAINING -> failed')\n",
        "\n",
        "def test_network(model, test_images, test_labels, labels_count, labels_encoder):\n",
        "    tf.reset_default_graph()\n",
        "    input = tf.placeholder(tf.float32, shape=[1,IMAGE_SIZE,IMAGE_SIZE,CHANNELS], name='input')\n",
        "    _, prediction = build_network(input, labels_count)\n",
        "    loader = tf.train.Saver()\n",
        "    config = tf.ConfigProto(allow_soft_placement=True)\n",
        "    correct_predictions = 0\n",
        "    total_number = len(test_labels)\n",
        "    with tf.Session(config=config) as session:\n",
        "        loader.restore(session, model)\n",
        "        for s in range(total_number):\n",
        "            batch = get_test_batch(test_images, test_labels, labels_encoder, s)\n",
        "            predicted_value = session.run(prediction, feed_dict={input:batch[0]})\n",
        "            if np.squeeze(predicted_value) == np.argmax(batch[1], axis=-1):\n",
        "                correct_predictions += 1\n",
        "        error_rate = 100.0 * (total_number - correct_predictions) / total_number\n",
        "        print('TESTING -> model: {} | correct: {}/{} | error: {:.1f}%'.format(model.split('/')[-1], correct_predictions, total_number, error_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GRaBo8nS-FT"
      },
      "source": [
        "def query_network(model, images, labels, labels_count, labels_encoder, labels_decoder):\n",
        "    tf.reset_default_graph()\n",
        "    input = tf.placeholder(tf.float32, shape=[1,IMAGE_SIZE,IMAGE_SIZE,CHANNELS], name='input')\n",
        "    _, prediction = build_network(input, labels_count)\n",
        "    loader = tf.train.Saver()\n",
        "    config = tf.ConfigProto(allow_soft_placement=True)\n",
        "    total_number = len(labels)\n",
        "    with tf.Session(config=config) as session:\n",
        "        loader.restore(session, model)\n",
        "        for s in range(total_number):\n",
        "            batch = get_test_batch(images, labels, labels_encoder, s)\n",
        "            predicted_value = session.run(prediction, feed_dict={input:batch[0]})\n",
        "            show_image(prepare_image(test_images[s]))\n",
        "            if np.squeeze(predicted_value) == np.argmax(batch[1], axis=-1):\n",
        "                answer = labels_decoder(np.squeeze(predicted_value))\n",
        "                print(\"QUERY | ANSWER: http://nilf.it/{}\".format(answer))\n",
        "            else:\n",
        "                answer = labels_decoder(np.squeeze(predicted_value))\n",
        "                truth = labels_decoder(np.argmax(batch[1], axis=-1)[0])\n",
        "                print(\"QUERY | ANSWER: http://nilf.it/{} | TRUTH: http://nilf.it/{}\".format(answer, truth))\n",
        "\n",
        "# query_network(best_model, test_images, test_labels, labels_count, labels_encoder, labels_decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arTKHNzW7tRr"
      },
      "source": [
        "train_network(train_images, train_labels, labels_count, validation_images, validation_labels, labels_encoder)\n",
        "# resume_training(train_images, train_labels, labels_count, validation_images, validation_labels, labels_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8ROsAB4W0BW"
      },
      "source": [
        "test_network(last_model, test_images, test_labels, labels_count, labels_encoder)\n",
        "test_network(best_model, test_images, test_labels, labels_count, labels_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}